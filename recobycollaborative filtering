import math
from pyspark import SparkContext
from pyspark.mllib.recommendation import ALS
import math


# TO TUNE

ratings_file = "file:///home/cloudera/recommendation/ml-latest-small/ratings.csv"
movies_file = "file:///home/cloudera/recommendation/ml-latest-small/movies.csv"
seed = 5L
iterations = 10
regularization_parameter = 0.1
rank = 7
tolerance = 0.02
min_error= float('inf')

sc = SparkContext("local", "Reco Engine")
sc.setLogLevel("WARN")

temp_ratings_raw = sc.textFile(ratings_file)
temp_ratings_raw_header = temp_ratings_raw.take(1)[0]
print "==================================================="
print "Header Information of Ratings", temp_ratings_raw_header
print "==================================================="

ratings_rdd = temp_ratings_raw.filter(lambda line: line!=temp_ratings_raw_header)\
    .map(lambda line: line.split(",")).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()


temp_movies_raw = sc.textFile(movies_file)
temp_movies_raw_header = temp_movies_raw.take(1)[0]
print "==================================================="
print "Header Information of Movies", temp_movies_raw_header
print "==================================================="

movies_rdd = temp_movies_raw.filter(lambda line: line!=temp_movies_raw_header)\
    .map(lambda line: line.split(",")).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()

print ("Number of movies", movies_rdd.count())
print ("Number of ratings", ratings_rdd.count())
